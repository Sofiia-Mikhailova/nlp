{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe82f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43bdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba15df95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5b588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8916ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0eedd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184321a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ab698a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n",
      "lemma names: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_synset = wn.synsets(\"car\")\n",
    "print(\"synsets:\", word_synset)\n",
    "print(\"lemma names:\", word_synset[0].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3afd6725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "346b546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53563657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wheeled vehicle adapted to the rails of railroad'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "79ebf907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three cars had jumped the rails']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ee9ca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63f7d0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4be640d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce1f53ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6891300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4cd630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n"
     ]
    }
   ],
   "source": [
    "tree = wn.synsets(\"tree\")[0]\n",
    "paths = tree.hypernym_paths()\n",
    "for p in paths:\n",
    "  print([synset.name() for synset in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f93944fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b5a9c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13c81ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4e6d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "126a3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cf68241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original   : ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "Lemmatized : ['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_sentence = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # перетворюємо всі токени в нижній регістр\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # отримуємо частини мови для кожного токена\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    for token, tag in tagged_tokens:\n",
    "        # перетворюємо тег у формат для лематизатора\n",
    "        if tag.startswith('N'):\n",
    "            wn_tag = 'n'   # noun\n",
    "        elif tag.startswith('V'):\n",
    "            wn_tag = 'v'   # verb\n",
    "        else:\n",
    "            wn_tag = 'a'   # adjective/adverb (default)\n",
    "\n",
    "        # лематизуємо токен\n",
    "        lemma = lemmatizer.lemmatize(token, wn_tag)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "\n",
    "    return lemmatized_sentence\n",
    "\n",
    "tokens = twitter_samples.tokenized('positive_tweets.json')[50]\n",
    "lemmas = lemmatize_sentence(tokens)\n",
    "\n",
    "print(\"Original   :\", tokens)\n",
    "print(\"Lemmatized :\", lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cf041bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bef3edf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7153470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After:  ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # CODE_START\n",
    "        # приводимо до нижнього регістру\n",
    "        tok = token.lower()\n",
    "        # відкидаємо URL-и та mentions\n",
    "        if re.match(r'https?://\\S+', tok) or re.match(r'@\\w+', tok):\n",
    "            continue\n",
    "        # відкидаємо стоп-слова\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "        # відкидаємо чисто пунктуацію\n",
    "        if all(ch in string.punctuation for ch in tok):\n",
    "            continue\n",
    "        # вибираємо тег для лематизатора\n",
    "        if tag.startswith('N'):\n",
    "            wn_tag = 'n'\n",
    "        elif tag.startswith('V'):\n",
    "            wn_tag = 'v'\n",
    "        elif tag.startswith('J'):\n",
    "            wn_tag = 'a'\n",
    "        else:\n",
    "            wn_tag = 'n'\n",
    "        # лематизуємо і додаємо в результат\n",
    "        cleaned_tokens.append(lemmatizer.lemmatize(tok, wn_tag))\n",
    "        # CODE_END\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Тест\n",
    "from nltk.corpus import twitter_samples\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After: \", process_tokens(tweet_tokens[50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cce45df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# CODE_START\n",
    "\n",
    "# завантажуємо токени\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "# обробляємо всі твіти\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n",
    "\n",
    "# CODE_END\n",
    "\n",
    "# перевіримо результат на прикладі 500-го елементу\n",
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c1e230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    all_words = []\n",
    "    # CODE_START\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            all_words.append(token)\n",
    "    # CODE_END\n",
    "    return all_words\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "53ecf9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 найпоширеніших слів у позитивних твітах:\n",
      ":d: 658\n",
      "thanks: 383\n",
      "follow: 362\n",
      "u: 360\n",
      "love: 337\n",
      "get: 269\n",
      "good: 261\n",
      "thank: 258\n",
      "day: 245\n",
      "like: 231\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "#створюємо розподіл частот за всіма словами\n",
    "freq_dist = FreqDist(all_pos_words)\n",
    "\n",
    "# витягуємо 10 найчастіших\n",
    "top_10 = freq_dist.most_common(10)\n",
    "\n",
    "print(\"Топ-10 найпоширеніших слів у позитивних твітах:\")\n",
    "for word, count in top_10:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14d9d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'and', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_hashtags(tokens):\n",
    "\n",
    "    return [tok for tok in tokens if not re.match(r'#\\w+', tok)]\n",
    "\n",
    "# Приклад\n",
    "sample_tokens = ['I', 'love', '#Ukraine', 'and', '#OpenAI', '!']\n",
    "print(remove_hashtags(sample_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07f360a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After:  ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def process_tokens_with_synsets(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # перетворюємо в нижній регістр\n",
    "        tok = token.lower()\n",
    "\n",
    "        # відкидаємо URL-и, згадки (@mention) та хештеги\n",
    "        if (re.match(r'https?://\\S+', tok)\n",
    "            or tok.startswith('@')\n",
    "            or tok.startswith('#')):\n",
    "            continue\n",
    "\n",
    "        # відкидаємо стоп-слова\n",
    "        if tok in stop_words:\n",
    "            continue\n",
    "\n",
    "        # відкидаємо токени, що складаються лише з пунктуації\n",
    "        if all(ch in string.punctuation for ch in tok):\n",
    "            continue\n",
    "\n",
    "        # визначаємо частину мови для пошуку синсетів\n",
    "        if tag.startswith('N'):\n",
    "            wn_pos = wn.NOUN       # іменник\n",
    "        elif tag.startswith('V'):\n",
    "            wn_pos = wn.VERB       # дієслово\n",
    "        elif tag.startswith('J'):\n",
    "            wn_pos = wn.ADJ        # прикметник\n",
    "        elif tag.startswith('R'):\n",
    "            wn_pos = wn.ADV        # прислівник\n",
    "        else:\n",
    "            wn_pos = wn.NOUN       # за замовчуванням — іменник\n",
    "\n",
    "        # намагаємось знайти синсети та взяти першу лему\n",
    "        synsets = wn.synsets(tok, pos=wn_pos)\n",
    "        if synsets:\n",
    "            # беремо першу лему з першого синсету\n",
    "            lemma = synsets[0].lemmas()[0].name()\n",
    "        else:\n",
    "            # якщо немає синсету — залишаємо токен без змін\n",
    "            lemma = tok\n",
    "\n",
    "        cleaned_tokens.append(lemma)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "tweet = twitter_samples.tokenized('positive_tweets.json')[50]\n",
    "print(\"Before:\", tweet)\n",
    "print(\"After: \", process_tokens_with_synsets(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34685926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def semantic_distance(word1, word2, pos='n'):\n",
    "    \"\"\"\n",
    "    Обчислює семантичну відстань між двома словами на основі їх найнижчого спільного гіпероніма (Lowest Common Hypernym).\n",
    "    Відстань = довжина_шляху_до_кореня_1 + довжина_шляху_до_кореня_2 - 2 * довжина_шляху_до_спільного_предка\n",
    "    \"\"\"\n",
    "    # отримуємо синонімічні множини (синсети) для обох слів з заданою частиною мови\n",
    "    synsets1 = wn.synsets(word1, pos=pos)\n",
    "    synsets2 = wn.synsets(word2, pos=pos)\n",
    "\n",
    "    # якщо хоча б одного слова немає у WordNet — повертаємо None\n",
    "    if not synsets1 or not synsets2:\n",
    "        return None\n",
    "\n",
    "    # беремо перші (найбільш вживані) синсети для простоти\n",
    "    s1 = synsets1[0]\n",
    "    s2 = synsets2[0]\n",
    "\n",
    "    # отримуємо всі шляхи гіперонімів до кореня для кожного слова\n",
    "    paths1 = s1.hypernym_paths()\n",
    "    paths2 = s2.hypernym_paths()\n",
    "\n",
    "    # перетворюємо всі шляхи в множини синсетів\n",
    "    set1 = set(s for path in paths1 for s in path)\n",
    "    set2 = set(s for path in paths2 for s in path)\n",
    "\n",
    "    #знаходимо спільних предків (гіперонімів)\n",
    "    common = set1.intersection(set2)\n",
    "    if not common:\n",
    "        return float('inf')  # якщо спільного предка не знайдено — повертаємо нескінченність\n",
    "\n",
    "    # вибираємо найближчого до слів спільного предка (найменша відстань)\n",
    "    min_distance = float('inf')\n",
    "    for ancestor in common:\n",
    "        # d1 — відстань від слова1 до предка\n",
    "        d1 = min(len(p) - p.index(ancestor) - 1 for p in paths1 if ancestor in p)\n",
    "        # d2 — відстань від слова2 до предка\n",
    "        d2 = min(len(p) - p.index(ancestor) - 1 for p in paths2 if ancestor in p)\n",
    "        distance = d1 + d2\n",
    "        # оновлюємо мінімальну відстань\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "\n",
    "    return min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00cbd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print( semantic_distance('dog', 'cat') )    \n",
    "print( semantic_distance('apple', 'fruit') ) \n",
    "print( semantic_distance('qwerty', 'asdf') ) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
