{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25e99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook C:/Users/HP/Downloads/nlp3.ipynb to script\n",
      "[NbConvertApp] Writing 8750 bytes to C:\\Users\\HP\\Downloads\\nlp3.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script C:/Users/HP/Downloads/nlp3.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a191e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "synsets: [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original   : ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "4\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\HP\\Downloads\")  # якщо lab3_clean.py там\n",
    "from lab3_clean import positive_cleaned_tokens_list, negative_cleaned_tokens_list\n",
    "\n",
    "print(len(positive_cleaned_tokens_list))\n",
    "print(len(negative_cleaned_tokens_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da50d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc9536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b469241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7653333333333333\n",
      "Most Informative Features\n",
      "                     sad = True           Negati : Positi =     34.0 : 1.0\n",
      "                  arrive = True           Positi : Negati =     26.8 : 1.0\n",
      "                    glad = True           Positi : Negati =     22.8 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.0 : 1.0\n",
      "                follower = True           Positi : Negati =     17.7 : 1.0\n",
      "              appreciate = True           Positi : Negati =     17.3 : 1.0\n",
      "                    miss = True           Negati : Positi =     14.3 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.1 : 1.0\n",
      "               community = True           Positi : Negati =     13.2 : 1.0\n",
      "                   enjoy = True           Positi : Negati =     13.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65930129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87bf930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\HP\\Downloads\") \n",
    "\n",
    "from lab3_clean import process_tokens, positive_cleaned_tokens_list, negative_cleaned_tokens_list\n",
    "\n",
    "# Перевіримо імпорт\n",
    "print(len(positive_cleaned_tokens_list))\n",
    "print(len(negative_cleaned_tokens_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f93ad945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"the service was so bad\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3da814e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Negative\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Positive\n",
      "great service :  Positive\n",
      "they stole my money :  Negative\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(get_token_dict(custom_tokens))\n",
    "\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "159b2b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7980\n",
      "Most Informative Features\n",
      "                   solid = True           Positi : Negati =     19.5 : 1.0\n",
      "                  boring = True           Negati : Positi =     14.0 : 1.0\n",
      "                  stupid = True           Negati : Positi =     13.3 : 1.0\n",
      "                 engross = True           Positi : Negati =     12.7 : 1.0\n",
      "                 generic = True           Negati : Positi =     12.6 : 1.0\n",
      "                    mess = True           Negati : Positi =     12.3 : 1.0\n",
      "               enjoyable = True           Positi : Negati =     12.1 : 1.0\n",
      "             pretentious = True           Negati : Positi =     11.4 : 1.0\n",
      "                    loud = True           Negati : Positi =     11.1 : 1.0\n",
      "                   stale = True           Negati : Positi =     11.1 : 1.0\n",
      "\n",
      "Tweets Samples:\n",
      "Just saw the new movie, it was awesome! #blockbuster -> Positive\n",
      "Ugh, stuck in traffic again. Worst day ever. -> Negative\n",
      "\n",
      "Reddit Samples:\n",
      "I really enjoyed the discussion in the thread, some great insights. -> Negative\n",
      "Mods are ruining this subreddit with over-moderation. -> Negative\n",
      "\n",
      "General Samples:\n",
      "service is so bad -> Negative\n",
      "totally loved it -> Positive\n",
      "they stole my money -> Negative\n"
     ]
    }
   ],
   "source": [
    "# retrain_sst_classifier.py\n",
    "# Python script to retrain NLTK NaiveBayesClassifier on the Stanford Sentiment Treebank dataset\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "import sys\n",
    "\n",
    "# Import your Lab3 cleaning routines\n",
    "# Ensure lab3_clean.py is on your PYTHONPATH\n",
    "from lab3clean_ import process_tokens\n",
    "\n",
    "# 1. Download required NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Load SST dataset (labels 0-4) in CSV format with fields 'label' and 'cleaned_text'\n",
    "datasets = load_dataset('rohith2812/stanford-sentiment-treebank-dataset', split=None)\n",
    "# 'train' and 'validation' splits available\n",
    "all_data = concatenate_datasets([datasets['train'], datasets['validation']])\n",
    "\n",
    "# 3. Filter out neutral (label==2) and map to binary labels\n",
    "def map_label(lbl):\n",
    "    if lbl in [0, 1]: return 'Negative'\n",
    "    if lbl in [3, 4]: return 'Positive'\n",
    "    return None\n",
    "\n",
    "binary = []\n",
    "for ex in all_data:\n",
    "    label = map_label(ex['label'])\n",
    "    if label:\n",
    "        binary.append({'text': ex['cleaned_text'], 'sentiment': label})\n",
    "\n",
    "# 4. Shuffle and limit to 10000 examples\n",
    "random.shuffle(binary)\n",
    "binary = binary[:10000]\n",
    "\n",
    "# 5. Tokenize, clean with lab3, split into POS/NEG\n",
    "pos_tokens, neg_tokens = [], []\n",
    "for item in binary:\n",
    "    tokens = word_tokenize(item['text'])\n",
    "    cleaned = process_tokens(tokens)\n",
    "    if item['sentiment'] == 'Positive':\n",
    "        pos_tokens.append(cleaned)\n",
    "    else:\n",
    "        neg_tokens.append(cleaned)\n",
    "\n",
    "# 6. Prepare feature dicts\n",
    "\n",
    "def to_feat_dict(tokens):\n",
    "    return {tok: True for tok in tokens}\n",
    "\n",
    "pos_feats = [(to_feat_dict(toks), 'Positive') for toks in pos_tokens]\n",
    "neg_feats = [(to_feat_dict(toks), 'Negative') for toks in neg_tokens]\n",
    "all_feats = pos_feats + neg_feats\n",
    "random.shuffle(all_feats)\n",
    "\n",
    "# 7. Train/test split (7000/3000)\n",
    "train_set = all_feats[:7000]\n",
    "test_set  = all_feats[7000:10000]\n",
    "\n",
    "# 8. Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# 9. Evaluate\n",
    "acc = classify.accuracy(classifier, test_set)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# 10. Wrapper for new text\n",
    "\n",
    "def get_sentiment(text):\n",
    "    cleaned = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(to_feat_dict(cleaned))\n",
    "\n",
    "# 11. Examples across domains\n",
    "samples = {\n",
    "    'Tweets': [\n",
    "        \"Just saw the new movie, it was awesome! #blockbuster\",\n",
    "        \"Ugh, stuck in traffic again. Worst day ever.\",\n",
    "    ],\n",
    "    'Reddit': [\n",
    "        \"I really enjoyed the discussion in the thread, some great insights.\",\n",
    "        \"Mods are ruining this subreddit with over-moderation.\",\n",
    "    ],\n",
    "    'General': [\n",
    "        \"service is so bad\",\n",
    "        \"totally loved it\",\n",
    "        \"they stole my money\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for domain, texts in samples.items():\n",
    "    print(f\"\\n{domain} Samples:\")\n",
    "    for t in texts:\n",
    "        print(f\"{t} -> {get_sentiment(t)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d8bcf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Accuracy: 0.8131\n",
      "Top NB features:\n",
      "Most Informative Features\n",
      "                   solid = True           Positi : Negati =     21.7 : 1.0\n",
      "                    flat = True           Negati : Positi =     18.5 : 1.0\n",
      "             pretentious = True           Negati : Positi =     18.5 : 1.0\n",
      "                  stupid = True           Negati : Positi =     13.4 : 1.0\n",
      "               wonderful = True           Positi : Negati =     13.2 : 1.0\n",
      "                   awful = True           Negati : Positi =     12.7 : 1.0\n",
      "                 engross = True           Positi : Negati =     12.5 : 1.0\n",
      "                  boring = True           Negati : Positi =     11.3 : 1.0\n",
      "                 shallow = True           Negati : Positi =     11.3 : 1.0\n",
      "                    joke = True           Negati : Positi =     10.7 : 1.0\n",
      "BernoulliNB (sklearn) Accuracy: 0.8182\n",
      "\n",
      "LogisticRegression (sklearn) Accuracy: 0.7866\n",
      "\n",
      "Summary:\n",
      "  NLTK NB      : 0.8131\n",
      "  Sklearn NB   : 0.8182\n",
      "  Sklearn LR   : 0.7866\n"
     ]
    }
   ],
   "source": [
    "# compare_nb_lr_sklearn_wrapper.py\n",
    "# Compare NLTK NaiveBayes vs. LogisticRegression wrapped with NLTK's SklearnClassifier\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from lab3clean_ import process_tokens\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Download tokenizer if needed\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Load SST dataset and filter binary labels\n",
    "datasets = load_dataset('rohith2812/stanford-sentiment-treebank-dataset', split=None)\n",
    "all_data = concatenate_datasets([datasets['train'], datasets['validation']])\n",
    "\n",
    "def map_label(lbl):\n",
    "    if lbl in (0,1): return 'Negative'\n",
    "    if lbl in (3,4): return 'Positive'\n",
    "    return None\n",
    "\n",
    "binary = []\n",
    "for ex in all_data:\n",
    "    sentiment = map_label(ex['label'])\n",
    "    if sentiment:\n",
    "        tokens = word_tokenize(ex['cleaned_text'])\n",
    "        cleaned = process_tokens(tokens)\n",
    "        binary.append((cleaned, sentiment))\n",
    "\n",
    "random.shuffle(binary)\n",
    "binary = binary[:10000]\n",
    "\n",
    "# 2. Prepare feature sets for NLTK classifiers\n",
    "def to_feat(tokens):\n",
    "    return {tok: True for tok in tokens}\n",
    "\n",
    "feat_labels = [(to_feat(tokens), label) for tokens, label in binary]\n",
    "train_set = feat_labels[:7000]\n",
    "test_set  = feat_labels[7000:10000]\n",
    "\n",
    "# 3. Native NLTK Naive Bayes\n",
    "nb = NaiveBayesClassifier.train(train_set)\n",
    "nb_acc = classify.accuracy(nb, test_set)\n",
    "print(f\"NaiveBayes Accuracy: {nb_acc:.4f}\")\n",
    "print(\"Top NB features:\")\n",
    "nb.show_most_informative_features(10)\n",
    "\n",
    "# 4. NLTK wrapper for sklearn BernoulliNB (as a baseline)\n",
    "bb_clf = SklearnClassifier(BernoulliNB(binarize=False))\n",
    "bb_clf.train(train_set)\n",
    "bb_acc = classify.accuracy(bb_clf, test_set)\n",
    "print(f\"BernoulliNB (sklearn) Accuracy: {bb_acc:.4f}\\n\")\n",
    "\n",
    "# 5. NLTK wrapper for sklearn LogisticRegression\n",
    "lr_estimator = LogisticRegression(max_iter=1000)\n",
    "lr_clf = SklearnClassifier(lr_estimator)\n",
    "lr_clf.train(train_set)\n",
    "lr_acc = classify.accuracy(lr_clf, test_set)\n",
    "print(f\"LogisticRegression (sklearn) Accuracy: {lr_acc:.4f}\\n\")\n",
    "\n",
    "# 6. Comparison summary\n",
    "print(\"Summary:\")\n",
    "print(f\"  NLTK NB      : {nb_acc:.4f}\")\n",
    "print(f\"  Sklearn NB   : {bb_acc:.4f}\")\n",
    "print(f\"  Sklearn LR   : {lr_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9013df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Test Accuracy: 0.7980\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.77      0.79       384\n",
      "    Positive       0.79      0.82      0.81       408\n",
      "\n",
      "    accuracy                           0.80       792\n",
      "   macro avg       0.80      0.80      0.80       792\n",
      "weighted avg       0.80      0.80      0.80       792\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "This movie was absolutely fantastic, loved every moment.\n",
      " -> Negative\n",
      "Worst experience ever, will not recommend.\n",
      " -> Negative\n",
      "The plot was decent but acting was subpar.\n",
      " -> Negative\n"
     ]
    }
   ],
   "source": [
    "# pipeline_sst_nb_pipeline.py\n",
    "# Full example: sentiment classification on SST with an sklearn Pipeline\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load SST dataset (train+validation)\n",
    "datasets = load_dataset(\n",
    "    'rohith2812/stanford-sentiment-treebank-dataset', split=None\n",
    ")\n",
    "all_data = concatenate_datasets([datasets['train'], datasets['validation']])\n",
    "\n",
    "# 2. Map to binary labels and collect texts\n",
    "\n",
    "def map_label(lbl):\n",
    "    if lbl in (0, 1): return 'Negative'\n",
    "    if lbl in (3, 4): return 'Positive'\n",
    "    return None\n",
    "\n",
    "texts, labels = [], []\n",
    "for ex in all_data:\n",
    "    sentiment = map_label(ex['label'])\n",
    "    if sentiment:\n",
    "        texts.append(ex['cleaned_text'])\n",
    "        labels.append(sentiment)\n",
    "\n",
    "# 3. Shuffle & limit to 10k samples\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "combined = combined[:10000]\n",
    "texts, labels = zip(*combined)\n",
    "\n",
    "# 4. Train/test split (7000/3000)\n",
    "X_train, y_train = texts[:7000], labels[:7000]\n",
    "X_test,  y_test  = texts[7000:10000], labels[7000:10000]\n",
    "\n",
    "# 5. Build the sklearn Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=10000, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('chi2', SelectKBest(chi2, k=5000)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# 6. Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Predict & evaluate\n",
    "preds = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Pipeline Test Accuracy: {acc:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# 8. Sample predictions\n",
    "test_samples = [\n",
    "    \"This movie was absolutely fantastic, loved every moment.\",\n",
    "    \"Worst experience ever, will not recommend.\",\n",
    "    \"The plot was decent but acting was subpar.\",\n",
    "]\n",
    "print(\"\\nSample Predictions:\")\n",
    "for txt in test_samples:\n",
    "    print(f\"{txt}\\n -> {pipeline.predict([txt])[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b829e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revamped NB Accuracy: 0.7664141414141414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        max_features=15000,\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=custom_tokenizer,\n",
    "        binary=False\n",
    "    )),\n",
    "    ('clf', ComplementNB(alpha=0.3)),\n",
    "])\n",
    "\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "print(\"Revamped NB Accuracy:\", pipeline_nb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04a0b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7803030303030303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7460    0.7793    0.7623       358\n",
      "    Positive     0.8110    0.7811    0.7958       434\n",
      "\n",
      "    accuracy                         0.7803       792\n",
      "   macro avg     0.7785    0.7802    0.7790       792\n",
      "weighted avg     0.7816    0.7803    0.7806       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# your cleaning\n",
    "from lab3clean_ import process_tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return process_tokens(tokens)\n",
    "\n",
    "# 1. Load SST and binarize\n",
    "datasets = load_dataset('rohith2812/stanford-sentiment-treebank-dataset', split=None)\n",
    "all_data = concatenate_datasets([datasets['train'], datasets['validation']])\n",
    "\n",
    "def map_label(lbl):\n",
    "    if lbl in (0,1): return 'Negative'\n",
    "    if lbl in (3,4): return 'Positive'\n",
    "    return None\n",
    "\n",
    "texts, labels = [], []\n",
    "for ex in all_data:\n",
    "    s = map_label(ex['label'])\n",
    "    if s:\n",
    "        texts.append(ex['cleaned_text'])\n",
    "        labels.append(s)\n",
    "\n",
    "# 2. Shuffle & limit\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "combined = combined[:10000]\n",
    "texts, labels = zip(*combined)\n",
    "\n",
    "# 3. Train/test split\n",
    "X_train, y_train = texts[:7000], labels[:7000]\n",
    "X_test,  y_test  = texts[7000:], labels[7000:]\n",
    "\n",
    "# 4. Define your new NB pipeline\n",
    "pipeline_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        max_features=15000,\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=custom_tokenizer,\n",
    "        binary=False\n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.5)),\n",
    "])\n",
    "\n",
    "# 5. Train\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "\n",
    "# 6. Predict & evaluate\n",
    "preds = pipeline_nb.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(classification_report(y_test, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec856087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB Accuracy: 0.7840909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7537    0.8080    0.7799       375\n",
      "    Positive     0.8154    0.7626    0.7881       417\n",
      "\n",
      "    accuracy                         0.7841       792\n",
      "   macro avg     0.7846    0.7853    0.7840       792\n",
      "weighted avg     0.7862    0.7841    0.7842       792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from lab3clean_ import process_tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Custom tokenizer that uses your lemmatization pipeline\n",
    "def custom_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return process_tokens(tokens)\n",
    "\n",
    "# 1. Load SST and binarize labels\n",
    "datasets = load_dataset('rohith2812/stanford-sentiment-treebank-dataset', split=None)\n",
    "all_data = concatenate_datasets([datasets['train'], datasets['validation']])\n",
    "\n",
    "def map_label(lbl):\n",
    "    if lbl in (0,1): return 'Negative'\n",
    "    if lbl in (3,4): return 'Positive'\n",
    "    return None\n",
    "\n",
    "texts, labels = [], []\n",
    "for ex in all_data:\n",
    "    s = map_label(ex['label'])\n",
    "    if s:\n",
    "        texts.append(ex['cleaned_text'])\n",
    "        labels.append(s)\n",
    "\n",
    "# 2. Shuffle & take 10 000 samples\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "combined = combined[:10000]\n",
    "texts, labels = zip(*combined)\n",
    "\n",
    "# 3. Split 7 000 train / 3 000 test\n",
    "X_train, y_train = texts[:7000], labels[:7000]\n",
    "X_test,  y_test  = texts[7000:], labels[7000:]\n",
    "\n",
    "# 4. Define your ComplementNB pipeline\n",
    "pipeline_cnb = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        max_features=15000,\n",
    "        ngram_range=(1,2),\n",
    "        tokenizer=custom_tokenizer\n",
    "    )),\n",
    "    ('clf', ComplementNB(alpha=0.3)),\n",
    "])\n",
    "\n",
    "# 5. Train\n",
    "pipeline_cnb.fit(X_train, y_train)\n",
    "\n",
    "# 6. Predict & evaluate\n",
    "preds = pipeline_cnb.predict(X_test)\n",
    "print(\"ComplementNB Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(classification_report(y_test, preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
