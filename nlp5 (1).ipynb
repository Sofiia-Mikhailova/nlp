{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79689802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d9c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edc6dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантаження корпусу 'brown'...\n",
      "Завантажено 57340 речень.\n",
      "Набори даних: Тренувальний=40000 речень, Тестовий=10000 речень.\n",
      "Завдання 0 & 2 (Laplace Smoothed) \n",
      "Навчання моделі (N=3, UNK Threshold=1)...\n",
      "  Розмір словника (включаючи спецтокени, UNK): 23238\n",
      "  Загальна кількість токенів після обробки: 951437\n",
      "  Підраховано 309737 унікальних 2-грам.\n",
      "  Підраховано 627538 унікальних 3-грам.\n",
      "Perplexity обчислено для 218483 N-грам.\n",
      "Laplace Smoothed (k=1) Trigram Perplexity на тесті: 9457.72\n",
      "Завдання 1: Порівняння моделей (на тесті)\n",
      "Навчання моделі (N=2, UNK Threshold=1)...\n",
      "  Розмір словника (включаючи спецтокени, UNK): 23238\n",
      "  Загальна кількість токенів після обробки: 911437\n",
      "  Підраховано 309736 унікальних 2-грам.\n",
      "Perplexity обчислено для 218483 N-грам.\n",
      "Bigram (N=2, k=1) Perplexity на тесті: 1708.18\n",
      "Навчання моделі (N=3, UNK Threshold=1)...\n",
      "  Розмір словника (включаючи спецтокени, UNK): 23238\n",
      "  Загальна кількість токенів після обробки: 951437\n",
      "  Підраховано 309737 унікальних 2-грам.\n",
      "  Підраховано 627538 унікальних 3-грам.\n",
      "Perplexity обчислено для 218483 N-грам.\n",
      "Trigram (N=3, k=1) Perplexity на тесті: 9457.72\n",
      "\n",
      "Висновок Завдання 1: Модель з меншою Perplexity краще передбачає наступне слово.\n",
      "Завдання 2: Методи Interpolation та Stupid Backoff\n",
      "Навчання моделі (N=3, UNK Threshold=1)...\n",
      "  Розмір словника (включаючи спецтокени, UNK): 23238\n",
      "  Загальна кількість токенів після обробки: 951437\n",
      "  Підраховано 309737 унікальних 2-грам.\n",
      "  Підраховано 627538 унікальних 3-грам.\n",
      "Perplexity обчислено для 218483 N-грам.\n",
      "Stupid Backoff (N=3, alpha=0.4) Perplexity на тесті: 300.58\n",
      "Навчання моделі (N=3, UNK Threshold=1)...\n",
      "  Розмір словника (включаючи спецтокени, UNK): 23238\n",
      "  Загальна кількість токенів після обробки: 951437\n",
      "  Підраховано 309737 унікальних 2-грам.\n",
      "  Підраховано 627538 унікальних 3-грам.\n",
      "Perplexity обчислено для 218483 N-грам.\n",
      "Interpolated (N=3, lambdas=[0.1, 0.3, 0.6]) Perplexity на тесті: 386.30\n",
      "\n",
      "Висновок Завдання 2: Методи Interpolation та Backoff краще обробляють рідкісні N-грами\n",
      "Завдання 3: Генерація речень за моделями\n",
      " Генерація: 'the government' (макс. 20 токенів)\n",
      "Промпт: 'the government' -> 'The government advantage blindness smeared buns target hevin scrutiny expressive negligible clothing hikes retaliatory windham peppered ameaux whispering jump glaze persists backs'\n",
      " Генерація: 'machine learning is' (макс. 15 токенів)\n",
      "Промпт: 'machine learning is' -> 'Machine learning is pilgrims hopes 1960 castor reassured anecdotes overwhelmingly garson progressing clarity thru frankly exhibit slightly repetitions'\n",
      "\n",
      "Генерація за Stupid Backoff Model:\n",
      " Генерація: 'the government' (макс. 20 токенів)\n",
      "Промпт: 'the government' -> 'The government prosecutors were attempting to extend the analysis luck in the pantheon, the used gradations of war preacher will many'\n",
      " Генерація: 'new possibilities in' (макс. 15 токенів)\n",
      "Промпт: 'new possibilities in' -> 'New possibilities in the middle west observed agencies of government: there are the natural sciences of'\n",
      "\n",
      "Генерація за Interpolated Model:\n",
      " Генерація: 'and make intelligent' (макс. 20 токенів)\n",
      "Промпт: 'and make intelligent' -> 'And make intelligent, because a serious and must do, then slipped around the earth.'\n",
      " Генерація: 'this is a' (макс. 10 токенів)\n",
      "Промпт: 'this is a' -> 'This is a hearty appreciation of gave up.'\n",
      " Приклад використання UNK\n",
      "\n",
      "Генерація з промптом, що може вести до UNK (використовуючи Backoff):\n",
      " Генерація: 'the young' (макс. 10 токенів)\n",
      "Промпт: 'the young' -> 'The young, covers.'\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import re # для обробки токенів при генерації\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "#Спеціальні токени\n",
    "START_TOKEN = '<s>' # Початок речення\n",
    "END_TOKEN = '</s>' # Кінець речення\n",
    "UNK_TOKEN = '<unk>' # Невідоме слово\n",
    "\n",
    "def load_nltk_sentences(corpus_name, file_ids=None):\n",
    "    print(f\"Завантаження корпусу '{corpus_name}'...\")\n",
    "    try:\n",
    "        if corpus_name == 'brown':\n",
    "            sents = nltk.corpus.brown.sents()\n",
    "        elif corpus_name == 'gutenberg':\n",
    "            sents = nltk.corpus.gutenberg.sents(file_ids)\n",
    "        else:\n",
    "            raise ValueError(f\"Непідтримуваний корпус: {corpus_name}\")\n",
    "    except nltk.downloader.DownloadError:\n",
    "         print(f\"Помилка завантаження корпусу '{corpus_name}'.\")\n",
    "         return [] # повертаємо порожній список, якщо завантаження не вдалося\n",
    "\n",
    "    # переведення токенів у нижній регістр\n",
    "    sentences = [[token.lower() for token in sent] for sent in sents]\n",
    "    print(f\"Завантажено {len(sentences)} речень.\")\n",
    "    return sentences\n",
    "\n",
    "# Базовий клас N-грамної моделі (з UNK)\n",
    "class BaseNGramModel:\n",
    "    def __init__(self, n: int, unk_threshold: int = 1):\n",
    "        \"\"\"\n",
    "        n: максимальний рівень N-грамів.\n",
    "        unk_threshold: поріг частоти для UNK.\n",
    "        \"\"\"\n",
    "        if n < 1: raise ValueError(\"N >= 1\")\n",
    "        self.n = n\n",
    "        self.unk_threshold = unk_threshold\n",
    "\n",
    "        self._ngram_counts = {} # Лічильники {рівень: Counter}\n",
    "        self._word_counts_raw = Counter() # Сирі частоти слів (для визначення UNK)\n",
    "        self.vocabulary = set() # Словник моделі (включаючи спецтокени, UNK)\n",
    "        self._total_tokens = 0 # Загальна кількість токенів після UNK/спецтокенів\n",
    "\n",
    "    def fit(self, sentences: list[list[str]]):\n",
    "        print(f\"Навчання моделі (N={self.n}, UNK Threshold={self.unk_threshold})...\")\n",
    "\n",
    "        #Підрахунок сирих частот слів для визначення UNK\n",
    "        for sent in sentences: self._word_counts_raw.update(sent)\n",
    "        unk_words = {word for word, freq in self._word_counts_raw.items() if freq <= self.unk_threshold}\n",
    "\n",
    "        # Формування словника моделі\n",
    "        self.vocabulary.update({START_TOKEN, END_TOKEN, UNK_TOKEN})\n",
    "        self.vocabulary.update(word for word in self._word_counts_raw if word not in unk_words)\n",
    "        print(f\"  Розмір словника (включаючи спецтокени, UNK): {len(self.vocabulary)}\")\n",
    "\n",
    "        #Обробка речень: додавання START/END, заміна UNK\n",
    "        processed_sentences = []\n",
    "        unigram_counts = Counter() # Підрахунок уніграм після обробки\n",
    "        for sent in sentences:\n",
    "            processed_tokens = [self._map_token_to_vocab(token) for token in sent] # Мапінг на UNK\n",
    "            sentence_with_specials = [START_TOKEN] * (self.n - 1) + processed_tokens + [END_TOKEN]\n",
    "            processed_sentences.append(sentence_with_specials)\n",
    "            unigram_counts.update(sentence_with_specials)\n",
    "\n",
    "        # Зберігаємо уніграми (рівень 1) та загальну кількість токенів\n",
    "        self._ngram_counts[1] = unigram_counts\n",
    "        self._total_tokens = sum(self._ngram_counts[1].values())\n",
    "        print(f\"  Загальна кількість токенів після обробки: {self._total_tokens}\")\n",
    "\n",
    "        # Підрахунок N-грамів (рівні 2 до n)\n",
    "        for i in range(2, self.n + 1):\n",
    "            self._ngram_counts[i] = Counter()\n",
    "            for sent in processed_sentences:\n",
    "                # Перебираємо вікна довжиною i\n",
    "                for j in range(len(sent) - i + 1):\n",
    "                    ngram = tuple(sent[j : j + i])\n",
    "                    self._ngram_counts[i][ngram] += 1\n",
    "            print(f\"  Підраховано {len(self._ngram_counts[i])} унікальних {i}-грам.\")\n",
    "\n",
    "\n",
    "    def _map_token_to_vocab(self, token: str) -> str:\n",
    "        return token if token in self.vocabulary else UNK_TOKEN\n",
    "\n",
    "    def get_count(self, tokens: tuple) -> int:\n",
    "        if not tokens: return self._total_tokens # count(()) - загальна кількість токенів\n",
    "\n",
    "        n = len(tokens)\n",
    "        # Мапінг вхідних токенів на словник моделі\n",
    "        processed_tokens = tuple(self._map_token_to_vocab(token) for token in tokens)\n",
    "\n",
    "        if n == 1: return self._ngram_counts.get(1, {}).get(processed_tokens[0], 0)\n",
    "        elif n >= 2 and n <= self.n: return self._ngram_counts.get(n, {}).get(processed_tokens, 0)\n",
    "        else: return 0 # Рівень вищий за n\n",
    "\n",
    "    def prob(self, ngram: tuple) -> float:\n",
    "        raise NotImplementedError(\"Метод 'prob' має бути реалізований у похідному класі.\")\n",
    "\n",
    "    def perplexity(self, sentences: list[list[str]]) -> float:\n",
    "        N = 0       # Кількість токенів для оцінки\n",
    "        log_prob_sum = 0.0 # Сума логарифмів ймовірностей\n",
    "\n",
    "        for sent in sentences:\n",
    "            processed_tokens = [self._map_token_to_vocab(token) for token in sent]\n",
    "            sentence_with_specials = [START_TOKEN] * (self.n - 1) + processed_tokens + [END_TOKEN]\n",
    "            num_ngrams_in_sent = len(sentence_with_specials) - (self.n - 1)\n",
    "            if num_ngrams_in_sent <= 0: continue # Пропускаємо занадто короткі речення\n",
    "\n",
    "            N += num_ngrams_in_sent\n",
    "\n",
    "            # Обчислення логарифмів ймовірностей для кожної N-грами\n",
    "            for i in range(len(sentence_with_specials) - self.n + 1):\n",
    "                ngram = tuple(sentence_with_specials[i : i + self.n])\n",
    "                p = self.prob(ngram) # Використовуємо prob() конкретної моделі\n",
    "                if p == 0:\n",
    "                     return float('inf')\n",
    "\n",
    "                log_prob_sum += math.log(p)\n",
    "\n",
    "        if N == 0:\n",
    "            print(\"Попередження: Немає N-грамів для оцінки в тестовому наборі.\")\n",
    "            return float('inf') # Неможливо обчислити Perplexity\n",
    "\n",
    "        # Perplexity = exp(- (1/N) * log_prob_sum)\n",
    "        perplexity_value = math.exp(-log_prob_sum / N)\n",
    "        print(f\"Perplexity обчислено для {N} N-грам.\")\n",
    "        return perplexity_value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "         return f\"{self.__class__.__name__}(n={self.n}, unk_threshold={self.unk_threshold})\"\n",
    "\n",
    "\n",
    "#Модель зі згладжуванням Лапласа (add-k)\n",
    "class LaplaceSmoothedNGramModel(BaseNGramModel):\n",
    "    \"\"\"\n",
    "    N-грамна модель зі згладжуванням Лапласа (add-k).\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, smoothing_k: float = 1.0, unk_threshold: int = 1):\n",
    "        super().__init__(n, unk_threshold)\n",
    "        if smoothing_k < 0: raise ValueError(\"smoothing_k >= 0\")\n",
    "        self.smoothing_k = smoothing_k\n",
    "\n",
    "    def prob(self, ngram: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Обчислює P(w | context) = (count(ngram) + k) / (count(context) + k * |V|)\n",
    "        \"\"\"\n",
    "        if len(ngram) != self.n:\n",
    "            return 0.0\n",
    "\n",
    "        ngram_count = self.get_count(ngram) # Вже оброблено UNK всередині get_count\n",
    "\n",
    "        # Контекст - перші N-1 токенів N-грами\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.get_count(context) # Вже оброблено UNK всередині get_count\n",
    "\n",
    "        # Розмір словника для згладжування (включаючи UNK, START, END)\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        # Захист від ділення на нуль, хоча з k > 0 та vocab_size > 0 це можливо тільки якщо context_count + k*|V| = 0\n",
    "        # що малоймовірно, якщо словник не порожній.\n",
    "        denominator = context_count + self.smoothing_k * vocab_size\n",
    "        if denominator == 0:\n",
    "            return 0.0 if vocab_size > 0 else 0.0\n",
    "\n",
    "        return (ngram_count + self.smoothing_k) / denominator\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "         return f\"LaplaceSmoothedNGramModel(n={self.n}, k={self.smoothing_k}, unk_threshold={self.unk_threshold}, vocab_size={len(self.vocabulary)})\"\n",
    "\n",
    "\n",
    "# Модель з бекофом (Stupid Backoff)\n",
    "class StupidBackoffModel(BaseNGramModel):\n",
    "    \"\"\"\n",
    "    N-грамна модель з тупим бекофом.\n",
    "    P_bo(w|context) = count(context, w) / count(context) if count(context, w) > 0\n",
    "                    = alpha * P_bo(w|context[1:]) otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, alpha: float = 0.4, unk_threshold: int = 1):\n",
    "        \"\"\"\n",
    "        alpha: коефіцієнт дисконтування при бекофі.\n",
    "        \"\"\"\n",
    "        super().__init__(n, unk_threshold)\n",
    "        if not 0 <= alpha <= 1: raise ValueError(\"alpha має бути в [0, 1]\")\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def prob(self, ngram: tuple) -> float:\n",
    "        current_n = len(ngram)\n",
    "\n",
    "        # Базовий випадок: Unigram (рівень 1)\n",
    "        if current_n == 1:\n",
    "            word = ngram[0]\n",
    "            word_count = self.get_count((word,)) # count(word)\n",
    "            total_count = self.get_count(())   # Загальна кількість токенів\n",
    "            return word_count / total_count if total_count > 0 else 0.0\n",
    "\n",
    "        ngram_count = self.get_count(ngram)\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.get_count(context)\n",
    "\n",
    "        # Якщо N-грама знайдена, використовуємо MLE (без згладжування на цьому рівні)\n",
    "        if ngram_count > 0:\n",
    "             # Перевірка context_count > 0 для безпеки, хоча якщо ngram_count>0, context_count має бути >= ngram_count\n",
    "             return ngram_count / context_count if context_count > 0 else 0.0 # Якщо context_count=0, щось не так з даними/підрахунком\n",
    "        # Якщо N-грама не знайдена, бекоф до нижчого рівня\n",
    "        else:\n",
    "            # Бекоф до ngram без першого токена\n",
    "            return self.alpha * self.prob(ngram[1:]) # Рекурсивний виклик\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "         return f\"StupidBackoffModel(n={self.n}, alpha={self.alpha}, unk_threshold={self.unk_threshold}, vocab_size={len(self.vocabulary)})\"\n",
    "\n",
    "\n",
    "#Модель з лінійною інтерполяцією \n",
    "class InterpolatedNGramModel(BaseNGramModel):\n",
    "    \"\"\"\n",
    "    N-грамна модель з лінійною інтерполяцією.\n",
    "    P_interp(w|context) = sum(lambda_k * P_k(w|last k-1 context tokens)) for k=1..n\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, lambdas: list[float], unk_threshold: int = 1):\n",
    "        \"\"\"\n",
    "        lambdas: список коефіцієнтів інтерполяції для рівнів 1 до n.\n",
    "                 Сума lambdas має бути близька до 1.\n",
    "        \"\"\"\n",
    "        if len(lambdas) != n: raise ValueError(f\"Кількість коефіцієнтів lambda має дорівнювати n ({n}), отримано {len(lambdas)}.\")\n",
    "        if not math.isclose(sum(lambdas), 1.0): print(f\"Попередження: Сума коефіцієнтів lambda ({sum(lambdas):.4f}) не дорівнює 1.\")\n",
    "        if any(lam < 0 for lam in lambdas): raise ValueError(\"Коефіцієнти lambda мають бути невід'ємними.\")\n",
    "\n",
    "        super().__init__(n, unk_threshold)\n",
    "        self.lambdas = lambdas\n",
    "        self._base_model_ref = None # Буде посилатись на self після fit\n",
    "\n",
    "    def prob(self, ngram: tuple) -> float:\n",
    "        if len(ngram) != self.n:\n",
    "            print(f\"Попередження: ngram довжини {len(ngram)} подано в модель рівня {self.n}. Очікується довжина {self.n}.\")\n",
    "            return 0.0 \n",
    "\n",
    "        total_prob = 0.0\n",
    "        # Ітеруємо по рівнях N-грам від 1 до n\n",
    "        for k in range(1, self.n + 1):\n",
    "            # Беремо останні k токенів з ngram\n",
    "            sub_ngram = ngram[self.n - k :]\n",
    "            # Контекст для k-грами - перші k-1 токенів sub_ngram\n",
    "            sub_context = sub_ngram[:-1]\n",
    "            sub_word = sub_ngram[-1] # Слово, яке прогнозуємо\n",
    "\n",
    "            # Отримуємо count(sub_ngram) та count(sub_context) використовуючи загальні лічильники\n",
    "            sub_ngram_count = self.get_count(sub_ngram)\n",
    "            sub_context_count = self.get_count(sub_context)\n",
    "\n",
    "            # Обчислюємо MLE для k-грами\n",
    "            mle_k = sub_ngram_count / sub_context_count if sub_context_count > 0 else 0.0\n",
    "            # Якщо k=1 (уніграма), sub_context порожній, count(sub_context) = total_tokens\n",
    "            if k == 1:\n",
    "                 mle_k = sub_ngram_count / self.get_count(()) if self.get_count(()) > 0 else 0.0\n",
    "            # Індекси lambdas: lambdas[0] для 1-грами, lambdas[1] для 2-грами, ..., lambdas[n-1] для n-грами\n",
    "            total_prob += self.lambdas[k - 1] * mle_k\n",
    "\n",
    "        return total_prob\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "         return f\"InterpolatedNGramModel(n={self.n}, lambdas={self.lambdas}, unk_threshold={self.unk_threshold}, vocab_size={len(self.vocabulary)})\"\n",
    "\n",
    "\n",
    "#Завдання 3: Генерація речень\n",
    "def generate_sentence(model: BaseNGramModel, prompt: str = \"\", max_length: int = 20) -> str:\n",
    "    print(f\" Генерація: '{prompt}' (макс. {max_length} токенів)\")\n",
    "\n",
    "    # Токенізація та обробка промпта (UNK)\n",
    "    prompt_tokens = [model._map_token_to_vocab(token.lower()) for token in re.findall(r'\\w+|[.,!?;:\\'\"()-]', prompt)]\n",
    "\n",
    "    # Початкова послідовність для контексту (включаючи START токени)\n",
    "    num_start = max(0, model.n - 1 - len(prompt_tokens))\n",
    "    current_sequence = [START_TOKEN] * num_start + prompt_tokens\n",
    "\n",
    "    generated_tokens = list(prompt_tokens) # Зберігаємо тільки згенеровані (та промпт)\n",
    "\n",
    "    # Цикл генерації\n",
    "    for _ in range(max_length):\n",
    "        # Поточний контекст (останні N-1 токенів)\n",
    "        context = tuple(current_sequence[-(model.n - 1):])\n",
    "\n",
    "        # Обчислення ймовірностей для наступного слова\n",
    "        candidates_probs = {}\n",
    "        for word in model.vocabulary:\n",
    "            # Не генеруємо START\n",
    "            if word == START_TOKEN: continue\n",
    "            # Ймовірність N-грами (контекст + слово)\n",
    "            p = model.prob(context + (word,))\n",
    "            candidates_probs[word] = p\n",
    "\n",
    "        # Вибірка наступного слова за ймовірністю\n",
    "        words = list(candidates_probs.keys())\n",
    "        probs = list(candidates_probs.values())\n",
    "\n",
    "        # Нормалізація для random.choices\n",
    "        total_prob = sum(probs)\n",
    "        if total_prob == 0:\n",
    "            break # Неможливо обрати наступне слово\n",
    "\n",
    "        probs = [p / total_prob for p in probs]\n",
    "\n",
    "        try:\n",
    "            next_token = random.choices(words, weights=probs, k=1)[0]\n",
    "        except ValueError as e:\n",
    "             print(f\"Помилка вибірки: {e}. Проблеми з ймовірностями? Зупинка.\")\n",
    "             break\n",
    "\n",
    "        # Додаємо токен до послідовності та згенерованих\n",
    "        current_sequence.append(next_token)\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        # Зупинка при END_TOKEN\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "\n",
    "    display_tokens = [token for token in generated_tokens if token not in {START_TOKEN, END_TOKEN, UNK_TOKEN}]\n",
    "    generated_text = \" \".join(display_tokens)\n",
    "    # Проста обробка пунктуації (приєднуємо до попереднього слова)\n",
    "    generated_text = re.sub(r'\\s+([.,!?;:\\'\"()-])', r'\\1', generated_text)\n",
    "\n",
    "    return generated_text.capitalize()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sents = load_nltk_sentences('brown')\n",
    "    # Обмежуємо для швидкості виконання прикладу\n",
    "    if len(sents) > 50000: sents = sents[:50000]\n",
    "\n",
    "    # Розподіл на тренувальний та тестовий набори\n",
    "    random.seed(42) # Для відтворюваності\n",
    "    random.shuffle(sents)\n",
    "    split_idx = int(0.8 * len(sents))\n",
    "    train_sents, test_sents = sents[:split_idx], sents[split_idx:]\n",
    "    print(f\"Набори даних: Тренувальний={len(train_sents)} речень, Тестовий={len(test_sents)} речень.\")\n",
    "\n",
    "    #Завдання 0 & Частина 2: Побудова базової N-грам моделі зі згладжуванням \n",
    "    print(\"Завдання 0 & 2 (Laplace Smoothed) \")\n",
    "    # N=3 модель зі згладжуванням Лапласа (k=1) та UNK порогом 1\n",
    "    # UNK слова будуть замінені на <unk> під час fit()\n",
    "    laplace_model = LaplaceSmoothedNGramModel(n=3, smoothing_k=1.0, unk_threshold=1)\n",
    "    laplace_model.fit(train_sents)\n",
    "\n",
    "    # Оцінка на тестовому наборі\n",
    "    laplace_perplexity = laplace_model.perplexity(test_sents)\n",
    "    print(f\"Laplace Smoothed (k=1) Trigram Perplexity на тесті: {laplace_perplexity:.2f}\")\n",
    "\n",
    "\n",
    "    # Завдання 1: Порівняння бі- та триграмних моделей (за Perplexity на тесті) \n",
    "    print(\"Завдання 1: Порівняння моделей (на тесті)\")\n",
    "    # Тренуємо окремі бі- та триграмні моделі (зі згладжуванням для можливості оцінки)\n",
    "    # Використовуємо той самий UNK поріг для чесного порівняння\n",
    "    bigram_model_comp = LaplaceSmoothedNGramModel(n=2, smoothing_k=1.0, unk_threshold=1)\n",
    "    bigram_model_comp.fit(train_sents)\n",
    "    bigram_perplexity_test = bigram_model_comp.perplexity(test_sents)\n",
    "    print(f\"Bigram (N=2, k=1) Perplexity на тесті: {bigram_perplexity_test:.2f}\")\n",
    "\n",
    "    trigram_model_comp = LaplaceSmoothedNGramModel(n=3, smoothing_k=1.0, unk_threshold=1)\n",
    "    trigram_model_comp.fit(train_sents) # Вже зроблено вище як laplace_model, але повторимо для ясності порівняння\n",
    "    trigram_perplexity_test = trigram_model_comp.perplexity(test_sents)\n",
    "    print(f\"Trigram (N=3, k=1) Perplexity на тесті: {trigram_perplexity_test:.2f}\")\n",
    "\n",
    "    print(\"\\nВисновок Завдання 1: Модель з меншою Perplexity краще передбачає наступне слово.\")\n",
    "\n",
    "\n",
    "    # Завдання 2: Інші методи згладжування/бекофу (Stupid Backoff, Interpolation)\n",
    "    print(\"Завдання 2: Методи Interpolation та Stupid Backoff\")\n",
    "\n",
    "    # Модель Stupid Backoff (N=3)\n",
    "    # Ця модель НЕ використовує add-k. Її prob() реалізує логіку бекофу.\n",
    "    stupid_backoff_model = StupidBackoffModel(n=3, alpha=0.4, unk_threshold=1)\n",
    "    stupid_backoff_model.fit(train_sents)\n",
    "    stupid_backoff_perplexity = stupid_backoff_model.perplexity(test_sents)\n",
    "    print(f\"Stupid Backoff (N=3, alpha=0.4) Perplexity на тесті: {stupid_backoff_perplexity:.2f}\")\n",
    "\n",
    "\n",
    "    # Модель з лінійною інтерполяцією (N=3)\n",
    "    interpolation_lambdas = [0.1, 0.3, 0.6] # lambda_1, lambda_2, lambda_3\n",
    "    interpolated_model = InterpolatedNGramModel(n=3, lambdas=interpolation_lambdas, unk_threshold=1)\n",
    "    interpolated_model.fit(train_sents) # fit() базового класу підраховує всі лічильники для всіх рівнів\n",
    "    interpolated_perplexity = interpolated_model.perplexity(test_sents)\n",
    "    print(f\"Interpolated (N=3, lambdas={interpolation_lambdas}) Perplexity на тесті: {interpolated_perplexity:.2f}\")\n",
    "\n",
    "    print(\"\\nВисновок Завдання 2: Методи Interpolation та Backoff краще обробляють рідкісні N-грами\")\n",
    "\n",
    "\n",
    "    # Завдання 3: Генерація речень \n",
    "    print(\"Завдання 3: Генерація речень за моделями\")\n",
    "\n",
    "    # Генерація за моделлю зі згладжуванням Лапласа (LaplaceSmoothedNGramModel)\n",
    "    generated_laplace_1 = generate_sentence(laplace_model, prompt=\"the government\")\n",
    "    print(f\"Промпт: 'the government' -> '{generated_laplace_1}'\")\n",
    "\n",
    "    generated_laplace_2 = generate_sentence(laplace_model, prompt=\"machine learning is\", max_length=15)\n",
    "    print(f\"Промпт: 'machine learning is' -> '{generated_laplace_2}'\")\n",
    "\n",
    "    # Генерація за моделлю Stupid Backoff (StupidBackoffModel)\n",
    "    print(\"\\nГенерація за Stupid Backoff Model:\")\n",
    "    generated_backoff_1 = generate_sentence(stupid_backoff_model, prompt=\"the government\")\n",
    "    print(f\"Промпт: 'the government' -> '{generated_backoff_1}'\")\n",
    "\n",
    "    generated_backoff_2 = generate_sentence(stupid_backoff_model, prompt=\"new possibilities in\", max_length=15)\n",
    "    print(f\"Промпт: 'new possibilities in' -> '{generated_backoff_2}'\")\n",
    "\n",
    "    # Генерація за моделлю Interpolated (InterpolatedNGramModel)\n",
    "    print(\"\\nГенерація за Interpolated Model:\")\n",
    "    generated_interpolated_1 = generate_sentence(interpolated_model, prompt=\"and make intelligent\")\n",
    "    print(f\"Промпт: 'and make intelligent' -> '{generated_interpolated_1}'\")\n",
    "\n",
    "    generated_interpolated_2 = generate_sentence(interpolated_model, prompt=\"this is a\", max_length=10)\n",
    "    print(f\"Промпт: 'this is a' -> '{generated_interpolated_2}'\")\n",
    "\n",
    "    print(\" Приклад використання UNK\")\n",
    "    print(\"\\nГенерація з промптом, що може вести до UNK (використовуючи Backoff):\")\n",
    "    generated_unk = generate_sentence(stupid_backoff_model, prompt=\"the young\", max_length=10)\n",
    "    print(f\"Промпт: 'the young' -> '{generated_unk}'\") # 'young' може бути рідкісним\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffe6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
