{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e569776-713a-4815-b275-c11f9c11fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3f2146-ae4a-49a7-9718-07dc89e65daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.test.utils\n",
    "\n",
    "# Set file names for train and test data\n",
    "lee_train_file = gensim.test.utils.datapath('lee_background.cor')\n",
    "lee_test_file = gensim.test.utils.datapath('lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0b1ef3-b8cf-47c5-9998-2d76ba2278d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['the', 'national', 'road', 'toll', 'for', 'the', 'christmas', 'new', 'year', 'holiday', 'period', 'stands', 'at', 'eight', 'fewer', 'than', 'for', 'the', 'same', 'time', 'last', 'year', 'people', 'have', 'died', 'on', 'new', 'south', 'wales', 'roads', 'with', 'eight', 'fatalities', 'in', 'both', 'queensland', 'and', 'victoria', 'western', 'australia', 'the', 'northern', 'territory', 'and', 'south', 'australia', 'have', 'each', 'recorded', 'three', 'deaths', 'while', 'the', 'act', 'and', 'tasmania', 'remain', 'fatality', 'free'], [2]>\n",
      "['the', 'united', 'states', 'government', 'has', 'said', 'it', 'wants', 'to', 'see', 'president', 'robert', 'mugabe', 'removed', 'from', 'power', 'and', 'that', 'it', 'is', 'working', 'with', 'the', 'zimbabwean', 'opposition', 'to', 'bring', 'about', 'change', 'of', 'administration', 'as', 'scores', 'of', 'white', 'farmers', 'went', 'into', 'hiding', 'to', 'escape', 'round', 'up', 'by', 'zimbabwean', 'police', 'senior', 'bush', 'administration', 'official', 'called', 'mr', 'mugabe', 'rule', 'illegitimate', 'and', 'irrational', 'and', 'said', 'that', 'his', 're', 'election', 'as', 'president', 'in', 'march', 'was', 'won', 'through', 'fraud', 'walter', 'kansteiner', 'the', 'assistant', 'secretary', 'of', 'state', 'for', 'african', 'affairs', 'went', 'on', 'to', 'blame', 'mr', 'mugabe', 'policies', 'for', 'contributing', 'to', 'the', 'threat', 'of', 'famine', 'in', 'zimbabwe']\n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "\n",
    "print(train_corpus[2])\n",
    "print(test_corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86ed37d-7085-41e3-a5e3-2ee1aa47c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe8f13e-34a3-4814-9eac-e1854c318e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1859cf7-4b04-40e3-90cc-cbb6fa856411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47f1397-d751-454b-bf15-3ca2f97f3b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1775490b640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b467c27-0cb7-4b8f-949d-e024d9cff6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15804976 -0.2499155  -0.11658205  0.21830133  0.01257771 -0.05956412\n",
      "  0.11286413 -0.03740422 -0.1275653  -0.04258005  0.16517814  0.00358914\n",
      " -0.03623567 -0.06548166 -0.19615334 -0.2485502   0.18084258  0.18186264\n",
      "  0.2080098  -0.08744554 -0.08761725  0.02893791  0.21769735 -0.09310921\n",
      " -0.01198547 -0.14353824 -0.33309653 -0.01825209 -0.18215586 -0.08988088\n",
      "  0.3794694  -0.04330229  0.24006465  0.15158856  0.17017049  0.14419761\n",
      "  0.00641705 -0.17949751 -0.08991947 -0.05096073 -0.01911813 -0.00208266\n",
      " -0.12335294 -0.04781395  0.20238739  0.02927922 -0.12975693 -0.00910016\n",
      "  0.14824723  0.00612068]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02603f03-5754-4cc6-b581-02c231daafaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 292, 1: 8})\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31430512-2968-4147-b481-92176a7106a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (22): «the biowarfare expert under scrutiny in the anthrax attacks declared am not the anthrax killer and lashed out today against attorney general john ashcroft for calling him person of interest in the investigation for the second time in two weeks the scientist went before throng of reporters outside his lawyer office to profess his innocence and decry the attention from law enforcers that he contends has destroyed his life»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (23, 0.6996737122535706): «americans fears about airplane security continue to increase after man made it through two separate flights with loaded gun in his carry on luggage the man was finally stopped before boarding third plane in memphis the man had travelled from florida to atlanta and then atlanta to memphis he was attempting to board his return flight last night when he was stopped by security personnel for random check they discovered loaded mm beretta semi automatic pistol in his hand luggage the man acknowledged the gun was his and was released on bail there is no suggestion he was planning any sort of terrorist attack but his ability to complete two flights while carrying the weapon has again highlighted airline security problems the incident follows last week drama when man was able to board plane from paris to miami with explosives in his shoes»\n",
      "\n",
      "MEDIAN (256, 0.29661914706230164): «survey of literacy and mathematical skills of year old australian school students has shown some alarming trends in boys education the survey was part of study undertaken by the organisation for economic cooperation and development oecd and involved countries including the united states canada brazil and japan the head of the australian council for educational research professor geoff masters says although the overall australian results are very encouraging there are some alarming signs boys tend to have more negative attitudes to reading they read less often than girls they are less interested in reading narrative texts storybooks for example he said»\n",
      "\n",
      "LEAST (84, -0.0922265574336052): «it has been confirmed two asylum seekers at the woomera detention centre have mutilated themselves during the current unrest the department has confirmed two asylum seekers harmed themselves yesterday an ambulance was seen entering and leaving the facility at high speed but the director of nursing at woomera hospital says no one from the detention centre has been admitted there since monday night unrest continued at the facility overnight with collective voice of detainees chanting visa the immigration department has confirmed at least to detainees breached compound fencing into prohibited zone in the facility overnight twenty two buildings have been destroyed or damaged by fire in three days it is thought the cooler temperatures expected in woomera today may lead to heightened daytime detainee activity south australian police are expected to reveal details soon of their operations after three days of detainee riots meanwhile coalition of australian religious leaders is calling for greater intake of refugees leaders from christian islamic and jewish communities are meeting in melbourne in response to what they say is detention system out of step with religious values the reverend tim costello says it is important for christians to remember jesus was refugee he says australians need to look beyond the small number of trouble makers at the woomera centre if the taliban was such an evil government for us to go to war against and risk our boys dying then surely those fleeing that government deserve our compassion he said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b763e80-bf7a-4746-aa16-adc4094b39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastparquet) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastparquet) (2024.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fastparquet) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eec387d-06bd-46c0-acae-e9e52c955fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hp\\anaconda3\\lib\\site-packages (19.0.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim pandas datasets pyarrow nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee5d30-766a-41c0-9e04-76c85e498ce8",
   "metadata": {},
   "source": [
    "завдання 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ea138e-a576-4ea0-953c-3bcb8b2f7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (2024.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89577ce-22a7-4035-81c8-37921928e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версія Gensim: 4.3.3\n",
      "Перевірка Gensim FAST_VERSION пройдена.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\datasets--stanfordnlp--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 201290.39 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 213329.86 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 322835.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантажено 5000 тренувальних документів та 1000 тестових документів.\n",
      "Перший попередньо оброблений тренувальний документ: TaggedDocument<['i', 'rented', 'i', 'am', 'curious-yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u.s.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '``', 'controversial', \"''\", 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'what', 'kills', 'me', 'about', 'i', 'am', 'curious-yellow', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'s\", 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', '.', 'while', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', '.', 'even', 'ingmar', 'bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'america', '.', 'i', 'am', 'curious-yellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', '(', 'no', 'pun', 'intended', ')', 'of', 'swedish', 'cinema', '.', 'but', 'really', ',', 'this', 'film', 'does', \"n't\", 'have', 'much', 'of', 'a', 'plot', '.'], [0]>\n",
      "Ініціалізація моделі Doc2Vec з 16 робочими потоками...\n",
      "Словник побудовано, розмір словника: 12061\n",
      "\n",
      "Виведений вектор для 'це був справді дивовижний і чудовий кінематографічний досвід':\n",
      "[-2.2609914e-03  3.1840450e-03  5.1941872e-05  2.0457013e-03\n",
      "  1.5592951e-03 -3.4367957e-03  7.0052565e-04 -3.7905932e-03\n",
      "  5.4413022e-04 -5.9696136e-04]... (перші 10 розмірностей)\n",
      "\n",
      "Найбільш схожі документи в тренувальному наборі до нового речення:\n",
      "  Док ID 4488 (Схожість: 0.2043): \"The proverb \"Never judge a book by it's cover\", was coined as a warning to those who fail to look beneath the surface. <br /><br />As I viewed the artwork to,\"King of the Ants\" I instantly thought HOR...\"\n",
      "  Док ID 3327 (Схожість: 0.1906): \"Joe Don Baker is...Thomas Jefferson Geronimo, a pudgy, sweaty murderous oaf in a stupid cowboy suit that Roy Rogers would have laughed at. Somehow he still has a badge, probably because he lives in Te...\"\n",
      "  Док ID 1943 (Схожість: 0.1879): \"Trying to cash in on the success of Deal Or No Deal and 1 Versus 100 comes this lame excuse for entertainment - Show Me The Money, in which 12 sexy 'dancers' shimmy out in shiny red hooker attire. A c...\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from datasets import load_dataset # Для завантаження наборів даних з Hugging Face Hub\n",
    "import nltk # Для токенізації\n",
    "import multiprocessing # Для використання багатоядерності\n",
    "\n",
    "print(f\"Версія Gensim: {gensim.__version__}\")\n",
    "# Переконуємося, що доступна швидка C-версія Doc2Vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "print(\"Перевірка Gensim FAST_VERSION пройдена.\")\n",
    "\n",
    "nltk.download('punkt', quiet=True) # quiet=True, щоб зменшити кількість виводу, якщо все вже є\n",
    "\n",
    "dataset_train = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5000]\") # Перші 5000 зразків для тренування\n",
    "dataset_test = load_dataset(\"stanfordnlp/imdb\", split=\"test[:1000]\")   # Перші 1000 зразків для тестування\n",
    "print(f\"Завантажено {len(dataset_train)} тренувальних документів та {len(dataset_test)} тестових документів.\")\n",
    "\n",
    "train_corpus = []\n",
    "for i, item in enumerate(dataset_train):\n",
    "    text_document = item['text']\n",
    "    tokens = nltk.word_tokenize(text_document.lower())\n",
    "    train_corpus.append(TaggedDocument(tokens, [i]))\n",
    "\n",
    "if not train_corpus: # перевірка чи корпус не порожній\n",
    "    print(\"Тренувальний корпус порожній. Перевірте завантаження набору даних та попередню обробку.\")\n",
    "    pass # Дозволяємо програмі продовжити; помилка виникне пізніше, якщо корпус порожній.\n",
    "\n",
    "if train_corpus: # Виводимо приклад, тільки якщо корпус не порожній\n",
    "    print(f\"Перший попередньо оброблений тренувальний документ: {train_corpus[0]}\")\n",
    "\n",
    "\n",
    "# модель Doc2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(f\"Ініціалізація моделі Doc2Vec з {cores} робочими потоками...\")\n",
    "\n",
    "model = Doc2Vec(\n",
    "    vector_size=100,\n",
    "    min_count=5,\n",
    "    epochs=20,\n",
    "    dm=1,\n",
    "    workers=cores-1 if cores > 1 else 1\n",
    ")\n",
    "\n",
    "# Якщо train_corpus порожній, тут виникне помилка.\n",
    "model.build_vocab(train_corpus)\n",
    "print(f\"Словник побудовано, розмір словника: {len(model.wv.index_to_key)}\")\n",
    "\n",
    "model.train(\n",
    "    train_corpus,\n",
    "    total_examples=model.corpus_count,\n",
    "    epochs=model.epochs\n",
    ")\n",
    "\n",
    "model_save_path = \"imdb_doc2vec_model_ua.model\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "#  виведення вектора для нового речення \n",
    "new_doc_text = \"це був справді дивовижний і чудовий кінематографічний досвід\"\n",
    "new_doc_tokens = nltk.word_tokenize(new_doc_text.lower())\n",
    "\n",
    "inferred_vector = model.infer_vector(new_doc_tokens)\n",
    "print(f\"\\nВиведений вектор для '{new_doc_text}':\\n{inferred_vector[:10]}... (перші 10 розмірностей)\")\n",
    "\n",
    "# Перевіряємо, чи існує атрибут 'dv' і чи він не порожній, перш ніж викликати most_similar\n",
    "if hasattr(model, 'dv') and model.dv:\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=3)\n",
    "    print(\"\\nНайбільш схожі документи в тренувальному наборі до нового речення:\")\n",
    "    for doc_tag, similarity in sims:\n",
    "        original_doc_text = dataset_train[doc_tag]['text'][:200]\n",
    "        print(f\"  Док ID {doc_tag} (Схожість: {similarity:.4f}): \\\"{original_doc_text}...\\\"\")\n",
    "else:\n",
    "    print(\"\\nmodel.dv не заповнений або відсутній. Неможливо знайти схожі документи.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ad037-5353-4376-856a-f3ab2da9816a",
   "metadata": {},
   "source": [
    "завдання 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee742311-81d5-4a9d-a46f-d98d8f412756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запитний документ (ID: 511, Мітка: Негативний) \n",
      "I don't give much credence to AIDS conspiracy theories but its sociologically interesting to see the phenomenon dramatized. In the early years of the AIDS epidemic, the suffering and paranoia of the scared and dying often generated such dark fantasies. This was especially true in the politically radical and sexually extreme demi-monde of San Francisco. The city, renowned for its beauty, has rarely appeared uglier than in this film. A sense of darkness and decomposition pervades every scene.<br /...\n",
      " 5 найбільш схожих документів з тренувального набору моделі\n",
      "\n",
      "Документ (ID з model.dv: 511, Мітка: Негативний, Схожість: 0.9005):\n",
      "I don't give much credence to AIDS conspiracy theories but its sociologically interesting to see the phenomenon dramatized. In the early years of the AIDS epidemic, the suffering and paranoia of the scared and dying often generated such dark fantasies. This was especially true in the politically rad...\n",
      "\n",
      "Документ (ID з model.dv: 1119, Схожість: 0.6314):\n",
      "(Неможливо відобразити текст, оскільки ID виходить за межі поточного завантаженого `dataset_for_similarity`)\n",
      "Це означає, що схожий документ був у повному тренувальному наборі моделі, але не в поточній завантаженій підмножині.\n",
      "\n",
      "Документ (ID з model.dv: 951, Мітка: Негативний, Схожість: 0.5964):\n",
      "An intriguing premise of hand-drawn fantasy come to life in a child's fever dreams. However, I imagine the average nonfictional child is far more adept at scaring themselves than Bernard Rose is at riveting the viewer. The duel between Anna's two realities drags on far too long to sustain interest, ...\n",
      "\n",
      "Документ (ID з model.dv: 1256, Схожість: 0.5915):\n",
      "(Неможливо відобразити текст, оскільки ID виходить за межі поточного завантаженого `dataset_for_similarity`)\n",
      "Це означає, що схожий документ був у повному тренувальному наборі моделі, але не в поточній завантаженій підмножині.\n",
      "\n",
      "Документ (ID з model.dv: 2648, Схожість: 0.5875):\n",
      "(Неможливо відобразити текст, оскільки ID виходить за межі поточного завантаженого `dataset_for_similarity`)\n",
      "Це означає, що схожий документ був у повному тренувальному наборі моделі, але не в поточній завантаженій підмножині.\n",
      "\n",
      "Мітка запитного документа: Негативний\n",
      "Мітки знайдених схожих документів (з поточного `dataset_for_similarity`):\n",
      "  ID 511: Негативний\n",
      "  ID 951: Негативний\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "import random\n",
    "import os # Додано для перевірки існування файлу\n",
    "\n",
    "model_path = \"imdb_doc2vec_model_ua.model\" # Шлях до вашої збереженої моделі\n",
    "\n",
    "# Перевіряємо, чи існує файл моделі, перш ніж намагатися його завантажити\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Файл моделі '{model_path}' не знайдено.\")\n",
    "    exit() # Завершуємо роботу, якщо моделі немає\n",
    "\n",
    "model = Doc2Vec.load(model_path)\n",
    "\n",
    "dataset_for_similarity = load_dataset(\"stanfordnlp/imdb\", split=\"train[:1000]\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Токенізує текст та переводить його в нижній регістр\n",
    "def preprocess_text(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "#  випадковий документ з  набору даних\n",
    "random_doc_index = random.randint(0, len(dataset_for_similarity) - 1)\n",
    "query_document_text = dataset_for_similarity[random_doc_index]['text']\n",
    "query_document_label = dataset_for_similarity[random_doc_index]['label']\n",
    "\n",
    "print(f\"Запитний документ (ID: {random_doc_index}, Мітка: {'Позитивний' if query_document_label == 1 else 'Негативний'}) \")\n",
    "print(query_document_text[:500] + \"...\" if len(query_document_text) > 500 else query_document_text)\n",
    "\n",
    "query_tokens = preprocess_text(query_document_text)\n",
    "\n",
    "#отримання вектора для запитного документа за допомогою навченої моделі\n",
    "inferred_vector = model.infer_vector(query_tokens)\n",
    "\n",
    "#Пошук найбільш схожих документів у навчальному корпусі моделі\n",
    "# Перевіряємо, чи існує атрибут 'dv' і чи він не порожній\n",
    "if hasattr(model, 'dv') and model.dv:\n",
    "    num_similar = 5\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=num_similar)\n",
    "\n",
    "    print(f\" {num_similar} найбільш схожих документів з тренувального набору моделі\")\n",
    "    for doc_id_in_model_dv, similarity_score in sims:\n",
    "        # Перевіряємо, чи індекс знаходиться в межах поточного завантаженого набору\n",
    "        if doc_id_in_model_dv < len(dataset_for_similarity):\n",
    "            similar_doc_text = dataset_for_similarity[doc_id_in_model_dv]['text']\n",
    "            similar_doc_label = dataset_for_similarity[doc_id_in_model_dv]['label']\n",
    "            label_text = 'Позитивний' if similar_doc_label == 1 else 'Негативний'\n",
    "            print(f\"\\nДокумент (ID з model.dv: {doc_id_in_model_dv}, Мітка: {label_text}, Схожість: {similarity_score:.4f}):\")\n",
    "            print(similar_doc_text[:300] + \"...\" if len(similar_doc_text) > 300 else similar_doc_text)\n",
    "        else:\n",
    "            print(f\"\\nДокумент (ID з model.dv: {doc_id_in_model_dv}, Схожість: {similarity_score:.4f}):\")\n",
    "            print(\"(Неможливо відобразити текст, оскільки ID виходить за межі поточного завантаженого `dataset_for_similarity`)\")\n",
    "            print(\"Це означає, що схожий документ був у повному тренувальному наборі моделі, але не в поточній завантаженій підмножині.\")\n",
    "else:\n",
    "    print(\"model.dv не знайдено або порожній. Неможливо знайти схожі документи.\")\n",
    "\n",
    "\n",
    "#  перевірка, чи збігаються мітки (якщо доступні)\n",
    "print(f\"\\nМітка запитного документа: {'Позитивний' if query_document_label == 1 else 'Негативний'}\")\n",
    "if hasattr(model, 'dv') and model.dv and 'sims' in locals() and sims is not None:\n",
    "    print(\"Мітки знайдених схожих документів (з поточного `dataset_for_similarity`):\")\n",
    "    for doc_id_in_model_dv, _ in sims:\n",
    "        if doc_id_in_model_dv < len(dataset_for_similarity):\n",
    "            label = dataset_for_similarity[doc_id_in_model_dv]['label']\n",
    "            print(f\"  ID {doc_id_in_model_dv}: {'Позитивний' if label == 1 else 'Негативний'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
